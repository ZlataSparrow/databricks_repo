{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a59d36c7-f746-4fbd-917c-1ffce661e149",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load Data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2134a335-082f-400a-a197-4ee15b798afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.options(header=True).csv(\n",
    "    \"/Volumes/workspace/default/customer_churn/customer_churn_clean.csv\"\n",
    ")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c56449c-5d00-4df3-a70a-13fad8098142",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761339441078}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, lit\n",
    "\n",
    "cols_with_empty = [\n",
    "    c for c in data.columns\n",
    "    if data.filter(col(c) == \" \").count() > 0\n",
    "]\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame(\n",
    "        [Row(Columns_with_empty_string=cols_with_empty)]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d34e77-9103-4fed-ab0e-614b9f393a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = data.toDF(*[c.replace(\" \", \"\") for c in data.columns])\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4677d55-cb80-43e3-8d38-95882ae3defd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "data = data.withColumn(\n",
    "    \"TotalCharges\",\n",
    "    when(col(\"TotalCharges\") == \" \", \"0\").otherwise(col(\"TotalCharges\"))\n",
    ")\n",
    "malformed_rows = data.filter(col(\"TotalCharges\") == \"0\")\n",
    "display(malformed_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e00786-1c84-47cd-bd15-8dcf2851e254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(data[data[\"TotalCharges\"] == \"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67b9c2f6-1b0c-46b6-980e-9bd35fbfcc08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = data.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"float\"))\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2883b73d-892a-41ad-95ce-e42e37d9ca6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data tota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a20b2afe-077e-42fc-9edd-16150c78198e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Coerce/Fix data Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f2f486-e64c-4e23-9a1d-caf2ee3454d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Senior Citizen & Churn\n",
    "\n",
    "from pyspark.sql.types import BooleanType, ShortType, IntegerType\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "binary_columns = [\"SeniorCitizen\", \"ChurnValue\"]\n",
    "telco_customer_churn_df = data\n",
    "for column in binary_columns:\n",
    "    telco_customer_churn_df = data.withColumn(column, col(column).cast(BooleanType()))\n",
    "\n",
    "telco_customer_churn_df.select(*binary_columns).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba61538-83f6-468c-bec8-1cfeee28a8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Casting did not work for SC most probably becaus ethere was some null values or values which could npt be encoded correctly, we can forc using a somple filter method \n",
    "\n",
    "telco_customer_churn_df = telco_customer_churn_df.withColumn(\\\n",
    "    \"SeniorCitizen\", when(col(\"SeniorCitizen\") == \"Yes\", True ).otherwise(False))\n",
    "\n",
    "telco_customer_churn_df.select(\"SeniorCitizen\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a16c18-a6a7-450b-9923-2f0f1691c2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d00379e-9d53-4b78-9d7f-f2fac0cb2d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telco_customer_churn_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3353601e-c947-4a04-879a-068c8c927162",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Phone Service & Paperless Billing to new boolean using spark.sql and re-order columns\n",
    "\n",
    "telco_customer_churn_df.createOrReplaceTempView(\"telco_customer_churn_temp_view\")\n",
    "\n",
    "telco_customer_casted_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  CustomerID,\n",
    "  BOOLEAN(Dependents),\n",
    "  BOOLEAN(Partner),\n",
    "  BOOLEAN(InternetService),\n",
    "  BOOLEAN(PaperlessBilling),\n",
    "  *\n",
    "  EXCEPT(CustomerId, Dependents, Partner, InternetService, PaperlessBilling, ChurnValue),\n",
    "  ChurnValue\n",
    "FROM\n",
    "  telco_customer_churn_temp_view\n",
    "\"\"\")\n",
    "\n",
    "telco_customer_casted_df.select(\"Dependents\", \"Partner\", \"PaperlessBilling\", \"InternetService\").printSchema()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e24c45-72d9-4fc0-acd6-bfee8d33674c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast TenureMonths to Long and replace the original column\n",
    "telco_customer_casted_df = telco_customer_churn_df.selectExpr(\n",
    "    \"* except(TenureMonths)\",\n",
    "    \"cast(TenureMonths as long) as TenureMonths\"\n",
    ")\n",
    "telco_customer_casted_df.select(\"TenureMonths\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e926e487-2c40-4f63-b4d3-48c7251951a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Handling Outliers\n",
    "\n",
    "We will see how to handle outliers in column by identifying and addressing data points that fall far outside the typical range of values in a dataset. Common methods for handling outliers include removing them, filtering, transforming the data, or replacing outliers with more representative values.\n",
    "\n",
    "Follow these steps for handling outliers:\n",
    "\n",
    "\t•\tCreate a new silver table named as telco_customer_full_silver by appending silver to the original table name and then accessing it using Spark SQL.\n",
    "\t•\tFiltering out outliers from the TotalCharges column by removing rows where the column value exceeds the specified cutoff value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa2853e-ec2c-4bcb-89ab-7d09cdf2a884",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761336521205}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dGVsY29fY3VzdG9tZXJfY2FzdGVkX2RmID0gdGVsY29fY3VzdG9tZXJfY2FzdGVkX2RmLndpdGhDb2x1bW4oIlRvdGFsQ2hhcmdlcyIsIGNvbCgiVG90YWxDaGFyZ2VzIikuY2FzdCgibG9uZyIpKQp0ZWxjb19jdXN0b21lcl9jYXN0ZWRfZGYuc2VsZWN0KCJUb3RhbENoYXJnZXMiLCAiVGVudXJlTW9udGhzIikuZGlzcGxheSgp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView9ee39b0\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView9ee39b0\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView9ee39b0\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView9ee39b0) ,min_max AS (SELECT `TotalCharges`,(SELECT MAX(`TotalCharges`) FROM q) `target_column_max`,(SELECT MIN(`TotalCharges`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `TotalCharges`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 14 `step` FROM min_max) SELECT IF(ISNULL(`TotalCharges`),NULL,LEAST(WIDTH_BUCKET(`TotalCharges`,`min_value`,`max_value`,14),14)) `TotalCharges_BIN`,FIRST(`min_value` + ((IF(ISNULL(`TotalCharges`),NULL,LEAST(WIDTH_BUCKET(`TotalCharges`,`min_value`,`max_value`,14),14)) - 1) * `step`)) `TotalCharges_BIN_LOWER_BOUND`,FIRST(`step`) `TotalCharges_BIN_STEP`,COUNT(`TotalCharges`) `COUNT` FROM histogram_meta GROUP BY `TotalCharges_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView9ee39b0\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "TotalCharges",
             "id": "column_a599ecd9204"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 14,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "TenureMonths": {
             "type": "histogram",
             "yAxis": 0
            },
            "TotalCharges": {
             "type": "histogram",
             "yAxis": 0
            },
            "column_a599ecd9205": {
             "name": "TenureMonths",
             "type": "histogram",
             "yAxis": 0
            },
            "column_a599ecd9206": {
             "type": "histogram",
             "yAxis": 0
            },
            "column_a599ecd9207": {
             "type": "histogram",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "dfc0569d-161c-4e1f-bc4c-fc3742e70f5e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.53125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "TotalCharges_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "TotalCharges_BIN",
           "args": [
            {
             "column": "TotalCharges",
             "type": "column"
            },
            {
             "number": 14,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "TotalCharges_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "TotalCharges",
             "type": "column"
            },
            {
             "number": 14,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "TotalCharges_BIN_STEP",
           "args": [
            {
             "column": "TotalCharges",
             "type": "column"
            },
            {
             "number": 14,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "TotalCharges",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dGVsY29fY3VzdG9tZXJfY2FzdGVkX2RmID0gdGVsY29fY3VzdG9tZXJfY2FzdGVkX2RmLndpdGhDb2x1bW4oIlRvdGFsQ2hhcmdlcyIsIGNvbCgiVG90YWxDaGFyZ2VzIikuY2FzdCgibG9uZyIpKQp0ZWxjb19jdXN0b21lcl9jYXN0ZWRfZGYuc2VsZWN0KCJUb3RhbENoYXJnZXMiLCAiVGVudXJlTW9udGhzIikuZGlzcGxheSgp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewd00c760\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewd00c760\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewd00c760\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewd00c760) SELECT `TenureMonths`,`TotalCharges` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewd00c760\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 2",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "TenureMonths",
             "id": "column_a599ecd9212"
            },
            "y": [
             {
              "column": "TotalCharges",
              "id": "column_a599ecd9216"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "TenureMonths": {
             "type": "scatter",
             "yAxis": 0
            },
            "TotalCharges": {
             "type": "scatter",
             "yAxis": 0
            },
            "column_a599ecd9211": {
             "name": "TenureMonths",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "ac71b067-b5c5-4b2d-bf76-29fa9e0b589f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 9.53125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "TenureMonths",
           "type": "column"
          },
          {
           "column": "TotalCharges",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "telco_customer_casted_df = telco_customer_casted_df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(\"long\"))\n",
    "telco_customer_casted_df.select(\"TotalCharges\", \"TenureMonths\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec32584-5c93-44cd-aa71-f5c0c45ef133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# even thouh my dataset is pretty cleaned, best practices:\n",
    "\n",
    "TotalCharges_cutoff = 0\n",
    "\n",
    "# Use .filter nethod and SQL col() function\n",
    "\n",
    "telco_no_outliers_df =  telco_customer_casted_df.filter(\\\n",
    "    (col(\"TotalCharges\") > TotalCharges_cutoff)|\\\n",
    "    (col(\"TotalCharges\").isNull())) # Keep Nulls\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9ac1cf-5400-48e8-a7c5-da4419485cb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Removing outliers from PaymentMethod\n",
    "\n",
    "\t•\tIdentify the two lowest occurrence groups in the PaymentMethod column and calculate the total count and average MonthlyCharges for each group.\n",
    "\t•\tRemove customers from the identified low occurrence groups in the PaymentMethod column to filter out outliers.\n",
    "\t•\tCreate a new dataframe telco_filtered_df containing the filtered data.\n",
    "\t•\tCompare the count of records before and after by dividing the count of telco_casted_full_df and telco_no_outliers_df dataframes, removing outliers, and then materializing the resulting dataframe as a new table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5019b1-cb6b-4f50-be7d-587bb6270159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZnJvbSBweXNwYXJrLnNxbC5mdW5jdGlvbnMgaW1wb3J0IGNvbCwgY291bnQsIGF2ZwoKIyBJZGVudGlmeSAyIGxvd2VzdCBncm91cCBvY2N1cnJlbmNlcwoKZ3JvdXBfdmFyID0gIlBheW1lbnRNZXRob2QiCnN0YXRzX2RmID0gdGVsY29fbm9fb3V0bGllcnNfZGYuZ3JvdXBCeShncm91cF92YXIpIFwKICAgICAgICAgICAgICAgICAgICAgICAgLmFnZyhjb3VudCgiKiIpLmFsaWFzKCJUb3RhbCIpLCBcCiAgICAgICAgICAgICAgICAgICAgICAgICAgICBhdmcoIk1vbnRobHlDaGFyZ2VzIikuYWxpYXMoIk1vbnRobHlDaGFyZ2VzIikpIFwKICAgICAgICAgICAgICAgICAgICAgICAgLm9yZGVyQnkoY29sKCJUb3RhbCIpLmFzYygpKSBcCiAgICAgICAgICAgICAgICAgICAgICAgIC5saW1pdCgyKQoKIyBkaXNwbGF5ICAgICAgICAgICAgICAgICAgICAgICAgCmRpc3BsYXkoc3RhdHNfZGYp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView26657c4\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView26657c4\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView26657c4\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView26657c4) SELECT `PaymentMethod`,SUM(`Total`) `column_a599ecd9238` FROM q GROUP BY `PaymentMethod`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView26657c4\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "PaymentMethod",
             "id": "column_a599ecd9241"
            },
            "y": [
             {
              "column": "Total",
              "id": "column_a599ecd9238",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_a599ecd9238": {
             "name": "MonthlyCharges",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": true,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "SUM(Total)"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "69ac6742-3d8d-476f-98e9-0364132cbc36",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 11.53125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "PaymentMethod",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "PaymentMethod",
           "type": "column"
          },
          {
           "alias": "column_a599ecd9238",
           "args": [
            {
             "column": "Total",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, avg\n",
    "\n",
    "# Identify 2 lowest group occurrences\n",
    "\n",
    "group_var = \"PaymentMethod\"\n",
    "stats_df = telco_no_outliers_df.groupBy(group_var) \\\n",
    "                        .agg(count(\"*\").alias(\"Total\"), \\\n",
    "                            avg(\"MonthlyCharges\").alias(\"MonthlyCharges\")) \\\n",
    "                        .orderBy(col(\"Total\").asc()) \\\n",
    "                        .limit(2)\n",
    "\n",
    "# display                        \n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7296f78c-8be9-4f28-97e7-684f9b6722be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Gather 2 lowest groups name assuming count threshold is below 20% of full dataset and monthly charges < $20\n",
    "N = telco_no_outliers_df.count()  # total count\n",
    "lower_groups = [elem[group_var] for elem in stats_df.head(2) if elem['Total']/N < 0.2 and elem['MonthlyCharges'] < 20]\n",
    "print(f\"Removing groups: {', '.join(lower_groups)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4777ee50-a48f-4703-9375-b133be760cae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter/Remove listings from these low occurrence groups while keeping null occurrences\n",
    "telco_no_outliers_df = telco_no_outliers_df.filter(\n",
    "    ~col(group_var).isin(lower_groups) |\n",
    "    col(group_var).isNull()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3602e30-d845-4e62-809c-a2061d0401de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count/Compare datasets before/after removing outliers\n",
    "print(f\"Count - Before: {telco_customer_casted_df.count()} / After: {telco_no_outliers_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb57ac25-ecc9-4f8b-9ac1-428beebb25cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Materialize/Snap table [OPTIONAL/for instructor only]\n",
    "telco_no_outliers_df.write.mode(\"overwrite\").saveAsTable(\"telco_customer_full_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d054b15c-c509-4d47-8664-54972f80bdf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Handling Missing Values\n",
    "\n",
    "To handle missing values in a dataset, we need to identify columns with high percentages of missing data and drop those columns. Then, it removes rows with missing values. Numeric columns are imputed with 0, and string columns are imputed with ‘N/A’. Overall, the code demonstrates a comprehensive approach to handling missing values in the dataset.\n",
    "\n",
    "⸻\n",
    "\n",
    "Delete Columns\n",
    "\n",
    "\t•\tCreate a DataFrame called missing_df to count the missing values per column in the telco_no_outliers_df dataset.\n",
    "\t•\tThe missing_df DataFrame is then transposed for better readability using the TransposeDF function, which allows for easier analysis of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec8ef691-0503-4554-aa8c-815474d55757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "def calculate_missing(input_df, show=True):\n",
    "    \"\"\"\n",
    "    Helper function to calculate and display missing data\n",
    "    \"\"\"\n",
    "    missing_df = input_df.select([\n",
    "        count(\n",
    "            when(\n",
    "                (col(c).isNull()) | (col(c) == ' ') | (col(c) == '') | (col(c) == 'NULL') | (col(c) == 'None'),\n",
    "                c\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in input_df.columns\n",
    "    ])\n",
    "\n",
    "    missing_df_out = missing_df.selectExpr(\n",
    "        \"stack({0}, {1}) as (Column, Number_of_Missing_Values)\".format(\n",
    "            len(missing_df.columns),\n",
    "            ', '.join([f\"'{c}', `{c}`\" for c in missing_df.columns])\n",
    "        )\n",
    "    ).withColumn(\"Number_of_Missing_Values\", col(\"Number_of_Missing_Values\").cast(\"long\"))\n",
    "\n",
    "    if show:\n",
    "        display(missing_df_out.orderBy(\"Number_of_Missing_Values\", ascending=False))\n",
    "\n",
    "    return missing_df_out\n",
    "\n",
    "missing_df = calculate_missing(telco_no_outliers_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Data_Imputation_&_Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
